#  Modern Data Lakehouse with Apache Iceberg

A production-grade, containerized Data Lakehouse environment featuring Apache Iceberg, Spark Cluster, Kafka, Trino, and Gravitino REST Catalog. Built with modern data engineering best practices and optimized for local development and testing.

## ğŸ“ Architecture

![Data Lakehouse Architecture](assets/architecture.png)

The architecture follows a layered approach:
- **Ingestion Layer**: Apache Kafka for real-time data streaming
- **Compute Layer**: Apache Spark cluster (1 Master + 2 Workers) for distributed processing
- **Query Layer**: Trino for fast, interactive SQL analytics
- **Catalog Layer**: Apache Gravitino as the Iceberg REST catalog service
- **Table Format**: Apache Iceberg for ACID transactions and time travel
- **File Format**: Apache Parquet for efficient columnar storage
- **Storage Layer**: MinIO (S3-compatible) for scalable object storage
- **Observability**: Grafana and Prometheus for metrics and monitoring

## âœ¨ Key Features

- **ğŸ”„ ACID Transactions**: Full ACID support via Apache Iceberg with snapshot isolation
- **âš¡ Distributed Processing**: Spark cluster with 1 master and 2 worker nodes
- **ğŸ¯ Multi-Engine Access**: Query data using Spark SQL, Trino, or PySpark notebooks
- **ğŸ“Š Real-time Ingestion**: Kafka cluster (3-node KRaft) for streaming data pipelines
- **ğŸ” Unified Catalog**: Gravitino REST catalog for centralized metadata management
- **ğŸ“ˆ Built-in Monitoring**: Prometheus metrics with Grafana dashboards
- **ğŸ³ Fully Dockerized**: One-command deployment with Docker Compose
- **ğŸ” S3-Compatible Storage**: MinIO for cost-effective data lake storage

## ğŸ›  Technology Stack

| Component | Version | Purpose |
|-----------|---------|---------|
| **Apache Spark** | 3.5.5 | Distributed data processing engine |
| **Apache Iceberg** | 1.10.0 | Open table format for huge analytic datasets |
| **Apache Gravitino** | 1.1.0 | REST catalog service for lakehouse metadata |
| **Apache Kafka** | 3.9.0 | Distributed event streaming platform |
| **Trino** | 471 | Fast distributed SQL query engine |
| **MinIO** | 2025-09-07 | S3-compatible object storage |
| **PostgreSQL** | 16 | Catalog backend database |
| **Grafana** | 12.1.0 | Metrics visualization |
| **Prometheus** | 3.5.1 | Metrics collection and alerting |
| **Python** | 3.12.3 | Runtime for PySpark applications |
| **Java** | 17 | Runtime for JVM-based services |

## ğŸš€ Quick Start

### Prerequisites
- Docker Engine 20.10+ with Docker Compose
- At least 8GB RAM available for containers
- Ports available: 8080, 8090, 8888, 9091, 3000, 9000, 5432

### 1. Clone and Start

```bash
# Clone the repository
git clone https://github.com/dducsw/mp252.git
cd mp252

# Start all services
docker compose up --detach --build
```

2. **Initialize Schema**:
   ```shell
   docker exec -it spark-master spark-sql -f /opt/spark/apps/setup/create_schema.sql
   ```

3. **Run a Pipeline**:
   ```shell
   docker exec -it spark-master python /opt/spark/apps/pipelines/create_example_table.py
   ```

4. **Using Notebooks**:
   - Access **JupyterLab** at `http://localhost:8888`.
   - Your notebooks are saved in the `notebooks/` directory.
   - To start a PySpark session in a notebook:
     ```python
     from pyspark.sql import SparkSession
     spark = SparkSession.builder.getOrCreate()
     ```

## ğŸ” Querying Data
You can query tables using either Spark or Trino:

**Spark SQL:**
```shell
docker exec -it spark-master spark-sql
SELECT * FROM catalog_iceberg.schema_iceberg.table_iceberg;
```

**Trino CLI:**
```shell
docker exec -it trino trino --catalog catalog_iceberg --schema schema_iceberg
SELECT * FROM table_iceberg;
```

## ğŸ“ Project Structure

```
Project
â”‚
â”œâ”€â”€ ğŸ“„ docker-compose.yml              # Main orchestration file for all services
â”œâ”€â”€ ğŸ“„ .env                            # Environment variables and version configurations
â”œâ”€â”€ ğŸ“„ README.md                       # Project documentation
â”œâ”€â”€ ğŸ“„ .gitignore                      # Git ignore patterns
â”‚
â”œâ”€â”€ ğŸ“ assets/                         # Documentation assets
â”‚
â”œâ”€â”€ ğŸ“ infrastructure/                 # Service-specific configurations
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ common/                     # Shared initialization scripts
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ gravitino/                  # Apache Gravitino REST Catalog
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ spark/                      # Apache Spark Cluster
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ trino/                      # Trino Query Engine
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ kafka/                      # Apache Kafka 
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ minio/                      # MinIO S3-Compatible Storage
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ postgres/                   # PostgreSQL Metadata Store
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ grafana/                    # Grafana Monitoring
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ prometheus/                 # Prometheus Metrics Collection
â”‚
â”œâ”€â”€ ğŸ“ notebooks/                      # Jupyter Notebooks
â”‚   â””â”€â”€ (Your interactive PySpark notebooks)
â”‚
â”œâ”€â”€ ğŸ“ pipelines/                      # Data Processing Pipelines
â”‚   â””â”€â”€ create_example_table.py        # Sample Iceberg table creation
â”‚
â”œâ”€â”€ ğŸ“ scripts/                        # Utility Scripts
â”‚
â””â”€â”€ ğŸ“ setup/                          # Initial Setup Scripts
    â””â”€â”€ create_schema.sql              # Database schema initialization
```


**â­ Star this repository if you find it helpful!**

*Enhanced and maintained by [dducsw](https://github.com/dducsw). Based on the original project by [marcellinus-witarsah](https://github.com/marcellinus-witarsah/local-data-lakehouse-iceberg).*


